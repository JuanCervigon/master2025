{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1bIpmgb_0BgTRBn2M3kuQDJaDXBX_j4Hj",
      "authorship_tag": "ABX9TyPC4CycgfXB/WZsKCMtY7EG"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción\n",
        "\n",
        "En este cuaderno vamos a desarrollar un análisis de regresión múltiple.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AgWUvbOjybyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Leer el fichero de datos\n",
        "\n",
        "Un fichero se puede leer de varias formas. Nosotros vamos a ver una forma fácil y práctica\n",
        "\n",
        "Si sincronizamos el cuaderno de Google Colab con nuestro Drive tendremos acceso a todos los ficheros de Drive. Para ello abrimos el menu de la derecha de Colab, pulsamos en el en el icono `Archivos` y después en el icono `activar Drive`. El cuaderno quedará sincronizado con nuestro Drive automáticamente. La sincronización se ejecuta automáticamente cada vez que se abra el cuaderno.\n",
        "\n",
        "Una vez sincronizado, para acceder a un fichero concreto de Drive, tenemos que especificar la ruta de acceso. La ruta se puede copiar en el portapapeles abriendo el menu contextual en el fichero correspondiente (poner el cursor encima del fichero y pulsar en los tres puntos verticales para abrir el menu contextual)\n",
        "\n",
        "Una vez hemos sincronizado con Drive y tenemos la ruta de acceso al fichero, podemos leer los datos en un `DataFrame` de Pandas\n"
      ],
      "metadata": {
        "id": "O_GPoz6CBtPF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W-2Q0TTZaGd"
      },
      "outputs": [],
      "source": [
        "# Librerias\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Acceder a mi drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vXLP3yWUc19",
        "outputId": "7b931347-b5d4-4b65-e8de-8268eb187d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leer un fichero excel de my Google-Drive a una variable del cuaderno\n",
        "ruta = \"/content/drive/MyDrive/uc3m/Data/Advertising.xlsx\"\n",
        "df = pd.read_excel(ruta)\n"
      ],
      "metadata": {
        "id": "Ka6WkKvTZkKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Leer un fichero excel subido al cuaderno a una variable del cuaderno\n",
        "\n",
        "df = pd.read_excel('Advertising.xlsx')\n"
      ],
      "metadata": {
        "id": "Jnd383bUb1Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver el contenido del DataFrame df\n",
        "df\n"
      ],
      "metadata": {
        "id": "bG6ZbJnGHAhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ver las columnas del DataFrame\n",
        "df.columns"
      ],
      "metadata": {
        "id": "__pissSVaDt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspeccionar las variables\n",
        "\n",
        "* Visualizar los datos cruzando todas las variables dos a dos en una matriz de gráficos xy. La forma más sencilla de hacerlo es `Pandas`\n",
        "\n",
        "* Calcular la matriz de correlaciones para tener una medidad cuantitativa de la asociación lineal entre las variables"
      ],
      "metadata": {
        "id": "gawWySsE1lLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cruzar las variables dos a dos en una matriz de graficos xy\n",
        "# Solo hae falta Pandas\n",
        "pd.plotting.scatter_matrix(df);"
      ],
      "metadata": {
        "id": "k709gPzek-YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lo mismo pero más bonito con la librería Plotly\n",
        "# Cruzar las variables dos a dos en una matriz de graficos xy\n",
        "\n",
        "import plotly.express as px\n",
        "px.scatter_matrix(df)\n",
        "# Otra forma: guardar el gráfico en el objeto graf y luedo mostrarlo\n",
        "# graf = px.scatter_matrix(df)\n",
        "# graf.show()"
      ],
      "metadata": {
        "id": "KoZ6ZXs9hTmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular la matriz de correlaciones\n",
        "matriz_de_correlaciones=df.corr()\n",
        "matriz_de_correlaciones"
      ],
      "metadata": {
        "id": "MQCyNagMipia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hacer un gráfico heatmap de la mtriz de correlaciones\n",
        "# Hace falta matplotlib y seaborn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.heatmap(matriz_de_correlaciones);"
      ],
      "metadata": {
        "id": "_9KUcNPM1qTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora con el gráfico \"imshow()\" de \"matplotlib\"\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(matriz_de_correlaciones)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vCP0OJGaKlY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ahora con \"Matplotlib\" otra vez pero añadiendo\n",
        "# nombre de la variable, título del gráfico y correlaciones\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "variables =[\"Televisión\", \"Radio\", \"Prensa\", \"Ventas\"]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "im = ax.imshow(matriz_de_correlaciones)\n",
        "\n",
        "# Poner el nombre de las variables en el gráfico\n",
        "ax.set_xticks(np.arange(len(variables)), labels=variables)\n",
        "ax.set_yticks(np.arange(len(variables)), labels=variables)\n",
        "\n",
        "# Rotar los nombres del eje x para que quepan mejor\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "# Mostrar el valor de las correlaciones con dos decimales\n",
        "for i in range(len(variables)):\n",
        "    for j in range(len(variables)):\n",
        "        text = ax.text(j, i,round(matriz_de_correlaciones.iloc[i, j],2),\n",
        "                       ha=\"center\", va=\"center\", color=\"w\")\n",
        "\n",
        "# Poner un título al gráfico\n",
        "ax.set_title(\"Matriz de correlaciones\")\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lxip--OcL7eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo de regresión\n",
        "\n",
        "* Variable dependiente: ventas\n",
        "* Variables independientes: Gasto en publicidad en TV, Radio y Prensa\n",
        "\n",
        "Hacer un modelo de regresión lineal consiste en encontrar los parámetros \"betas\" que cumplan\n",
        "\n",
        "$β_0+β_1x_{i1}+\\dots+β_dx_{id}+e_i=\\hat{y_i}+e_i=y_i, \\quad i=1,\\dots,n$\n",
        "\n",
        "También podemos expresar el conjunto de las n ecuaciones en forma matricial\n",
        "\n",
        "$xβ+e=\\hat{y}+e={y}$\n",
        "\n",
        "Donde $x$ es la matriz de datos añadiendo un vector de unos a la izquierda y β es un vector con $d+1$ componentes\n",
        "\n",
        "Las \"betas\" se pueden determinar imponiendo las siguientes condiciones, todas ellas equivalentes\n",
        "\n",
        "\n",
        "* Por mínimos cuadrados, es decir, encontrar las \"betas\" que minimizan la suma de los errores al cuadrado:\n",
        "\n",
        " $\\sum{e_i^2}=\\sum{(\\hat{y_i}-y_i)^2}$\n",
        "\n",
        "* Proyectando ortogonalmente el punto $y$ en el subespacio vectorial generado por el vector de unos $(1,\\dots,1)$ y los vectores $x_i, i=1,\\dots,d$, es decir, encontrar las \"betas\" que definen la combinación lineal del punto del subespacio anterior $\\hat{y} $ más próximo a $y$\n",
        "\n",
        "* Máxima verosimilitud. Este método lo explicaremos más adelante con mayor detalle. Básicamente consiste en (i) suponer que los datos se ajustan a un modelo probabilístico paramétrico y (ii) determinar los parámetros de ese modelo maximizando una función llamada \"función de verosimilitud\". Las ecuaciones que hay que resolver para maximizar la verosimilitud son las mismas que hay que resolver para minimizar la suma de los errores al cuadrado. La ventaja de utilizar un modelo probabilístico reside en que todas las propiedades del modelo se pueden aplicar a los datos\n",
        "\n"
      ],
      "metadata": {
        "id": "OjbdustPpBNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mínimos cuadrados con `scikit learn`\n",
        "\n",
        "Utilizaremos el módulo `linear_model` de la librería `scikit-learn`\n",
        "\n",
        "Siempre utilicemos un modelo en Python, primero hay que \"instanciar\" el modelo y luego pasarle los datos. En este caso el modelo que vamos a \"instanciar\" es `LinearRegression()` y los datos que vamos a pasar al modelo serán los valores de las  variables independientes $x$ (es una matriz) y los valores de la variable dependiente $y$ (es un vector).\n",
        "\n",
        "La palabra \"instanciar\" se utiliza para indicar que hemos definido una instancia del modelo genérico dandole el nombre que nosotros queramos. Podemos tener varias instancias del mismo modelo y pasarle a cada instancia datos distintos\n",
        "\n"
      ],
      "metadata": {
        "id": "7hJVfGensZ0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Poner las variables independientes en la matriz x\n",
        "# Poner las variables   dependientes en el vector y\n",
        "\n",
        "x=np.array(df[['TV','radio','newspaper']])\n",
        "y=np.array(df['sales'])"
      ],
      "metadata": {
        "id": "aHVPHnvlE3Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model # importar el módulo de la librería\n",
        "mi_regresion = linear_model.LinearRegression() # \"instanciar\" el modelo\n",
        "mi_regresion.fit(x,y) # pasar los datos al modelo\n"
      ],
      "metadata": {
        "id": "qXZUe9SI_6ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener b1, b2 y b3\n",
        "mi_regresion.coef_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9bDCfUq8xZe",
        "outputId": "8dbd057f-1f34-4893-aabd-56a1cac99b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.04576465,  0.18853002, -0.00103749])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener b0, es decir, la intersección en el origen\n",
        "mi_regresion.intercept_"
      ],
      "metadata": {
        "id": "HJKv0SU_EHqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7097011-800e-4da3-810d-c738db982c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.938889369459412"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mínimos cuadrados\n",
        "\n",
        "Vamos a implementar el método de mínimos cuadrados definiendo nosotros todo o que necesitamos, es decir, sin utilizar rutinas que ya están definidas en Python\n",
        "\n",
        "Primero definimos la función RSS (Residual Sum of Squares) suma de errores al cuadrado. Esta función depende de las betas y de los datos\n",
        "\n",
        "Después buscamos los betas que minimizan esa función"
      ],
      "metadata": {
        "id": "ZstyFmHBEJKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación de la matriz de datos x,y en Numpy\n",
        "x=np.array(df[['TV','radio','newspaper']])\n",
        "y=np.array(df['sales'])\n",
        "x=np.c_[np.ones(len(x)),x] # añadir un vector de unos a la izquierda\n"
      ],
      "metadata": {
        "id": "JCOiEBZqlCJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x\n"
      ],
      "metadata": {
        "id": "xthJaPzpTGEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir la función RSS (suma de errores al cuadrado)\n",
        "def RSS(betas,x,y):\n",
        "    return sum((y-x@betas)**2)"
      ],
      "metadata": {
        "id": "Eool7ypjlWUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir unas betas iniciales para poder hacer cálculos\n",
        "betas=np.array([1,0,1,1])"
      ],
      "metadata": {
        "id": "kX1xia2tiRLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprobar la función RSS funciona\n",
        "RSS(betas,x,y)"
      ],
      "metadata": {
        "id": "IESkyopc7uGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Otra forma de calcular la suma de los errores al cuadrado\n",
        "(y-x@betas)@(y-x@betas)"
      ],
      "metadata": {
        "id": "fGS69BjGqydY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontrar los betas que minimizan la RSS\n",
        "from scipy.optimize import minimize\n",
        "optimo=minimize(RSS,betas,args=(x,y))\n",
        "betas=optimo.x\n",
        "betas"
      ],
      "metadata": {
        "id": "sfAJdkl7i29A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proyeccion ortogonal\n"
      ],
      "metadata": {
        "id": "FlXF1XicjfwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Poner las variables independientes en la matriz x\n",
        "# Poner las variables   dependientes en el vector y\n",
        "x=np.array(df[['TV','radio','newspaper']])\n",
        "y=np.array(df[['sales']])\n"
      ],
      "metadata": {
        "id": "Jlu7nqJkofyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos la matrix x añadiendo la columna con unos y quitando la y\n",
        "x=np.array(df) # crear el array a partir del DataFrame\n",
        "x=np.delete(x, 3, axis=1) # Quitar la variable depediente, es decir, quitar \"sales\", quitar la columna 3\n",
        "x=x=np.insert(x, 0,np.ones(len(x)) , axis=1) # Añadir un vector de unos\n",
        "x\n"
      ],
      "metadata": {
        "id": "5dpHQSeCjlc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculo de las betas del modelo betas=inversa(x'@x)@x'@y\n",
        "betas=np.linalg.inv(x.T@x)@x.T@y\n",
        "betas"
      ],
      "metadata": {
        "id": "oLY6EKEepig5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculo de la matriz proyección P= x@inversa(x'@x)@x'\n",
        "P=x@np.linalg.inv(x.T@x)@x.T"
      ],
      "metadata": {
        "id": "vRv9IIQBqUVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo de las predicciones y^=Py\n",
        "y_hat=P@y"
      ],
      "metadata": {
        "id": "gT_csM-BqwW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Por minimos cuadrados con statsmodels\n",
        "\n",
        "Es el método más \"estadístico\" porque estamos imponiendo un modelo estadistico a los datos.\n",
        "\n",
        "Estamos suponiendo que el error tiene una distribucuón normal\n",
        "\n",
        "Por lo tanho cada observación de y es un variable aleatoria normal conb media cero y desviación estándar sigma\n",
        "Todas las yi tienen la misma desv est\n",
        "\n",
        "Este modelo inicialmente puede parecer muy específoco, pero en general es muy robusto\n",
        "\n",
        "cuando la linealidad no funciona hay trucos para que funcione\n",
        "\n",
        "La ventaja de asumir el modelo: poder usar las rpopiedades del modelo\n",
        "\n"
      ],
      "metadata": {
        "id": "mdZ35STP-lBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importar la librería statsmodels\n",
        "import statsmodels.api as sm"
      ],
      "metadata": {
        "id": "RmKUagyA-ypm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pasar el DataFrame a la matrix x y el vector y\n",
        "x=np.array(df[['TV','radio','newspaper']])\n",
        "y=np.array(df['sales'])\n",
        "\n",
        "# Añadir el vector de unos a la matriz x. OJO! otra forma de hacerlo\n",
        "x = sm.add_constant(x)\n",
        "\n",
        "# \"Instanciar\" el modelo de regresión.\n",
        "# OJO! Al definir la instancia se pasan los datos\n",
        "modelo = sm.OLS(y,x)\n",
        "\n",
        "# Ajustar el modelo de regresión\n",
        "regre=modelo.fit()\n",
        "\n"
      ],
      "metadata": {
        "id": "xcA8OZRjQyGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(regre.summary())"
      ],
      "metadata": {
        "id": "vWQGF41TRiuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resultados de statsmodels\n",
        "* R-squared: $R^2=1-\\frac{RSS}{TSS}$, mide el porcentaje de varianza explicada por el modelo. Es un valor entre 0 y 1\n",
        "\n",
        "* F-statistis: $\\frac{MSS/k}{RSS/(n-k-1)}$ Es un estadístico que tiene una distribución de F de Snedecor bajo la hipótesis nula \"todos los coeficientes son cero\". Valores superiores a 3 o 4 implican que dicha hipótesis nula es falsa, por lo tanto se acepta la hipótesis alternativa: \"al menos algún coeficiente es distinto de cero\"\n",
        "\n",
        "* La tabla en la que se analizan los coeficientes de los regresores aparecen las siguientes columnas:\n",
        " * coef: valor estimado del coeficiente\n",
        " * std err: desviación típica del estimador del coeficiente\n",
        " * t, es el valor de la t de student que resulta bajo la hipótesis nula \"el coeficiente vale 0\". En la medida en que ese valor sea inferior a -2 o superior a 2, hay que rechazar la hipótesis nula.\n",
        " * p>|t|, es el p-valor de la t obtenida bajo la hipótesis nula \"el coeficiente vale cero\". En el ejemplo se puede descartar esa hipótesis para todos los coeficientes excepto para el coeficiente de x3\n",
        " * [0.025 - 0.975] es el intervalo de confianza para el coeficiente con un nivel de significación del 5% o con una confianza del 95%. Evidentemente, cuando el intervalo de confianza contiene el cero, no podemos rechazar la hipótesis nula de que el coeficiente es distinto de cero\n",
        "\n",
        "* Skew, el la asimitría de la distribución de los errores. Valores entre -2 y 2 serían tolerables\n",
        "\n",
        "* Kurtosis, es el grado de curtosis de la distribución de los errores. La curtosis miden si las colas de la distribución tienen más masa de probabilidad que las colas de la normal. Cuanto más cercano a cero sea la curtosis, mayor parecido a la distribución normal. Valores inferiores a -2 y superiores a 2 son excesivos pueden indicar una deviación significativa respecto de la normal\n",
        "\n",
        "* Durbin-Watson, es un test que mide la autocorrelación de los errores. Es relevante cuando las observaciones tienen una estructura temporal, es decir, cuando se observan de forma secuencial. Idealmente debe tomar el valor 2 o bien no desviarse más de un 15% de dicho valor. Cuando los datos no son secuenciales el estadístico es irrelevante\n",
        "\n",
        "* Jarque-Bera, es un estadístico para contrastar la normalidad de los errores. Tiene una distribución asintóticamente como una chi cuadrado con dos grados de libertad bajo la hipótesis nula de que los datos pertenecen a una distribución normal. Es un test de normalidad basado en la asimistria y la curtosis. Para determinar el nivel de confianza de la hipótesis nula se puede observar el p-valor Prob(JB) que aparece a continuación en la tabla\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6WOpVDEVbkwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Maxima verosimilitud\n",
        "\n",
        "Para aplicar el método de la máxima verosimilitud tenemos que imponer un modelo probabilístico y paramétrico a los datos y después calcular los parámtetros de esde modelo maximizando la función de verosimilitud\n",
        "\n",
        "\n",
        "El modelo probabilístico que imponemos a los datos es el siguiente:\n",
        "\n",
        "* Las observaciones de la variable dependiente tienen una distribución normal de media $x_iβ$ y varianza común $σ^2$. Además son independientes\n",
        "\n",
        "La afirmación anterior es equivalente a\n",
        "\n",
        "* Los datos verifican  $β_0+β_1x_i+\\dots+β_dx_i+e_i=y_i\\quad  i=1,\\dots,n$\n",
        "\n",
        "Los valores de las variables independientes $x_i$ son deterministas. Los errores $e_i$ son variables aleatorias y por lo tanto los valores de la variable dependiente $y_i$ son resultados de variables aleatorias.\n",
        "\n",
        "Podemos expresar lo anterior de cuaquiera de las siguientes formas equivalentes:\n",
        "\n",
        "\n",
        "* $e_i\\sim N(0,σ^2)$\n",
        "* $y_i\\sim N(x_iβ,σ^2)$\n",
        "\n",
        "Dado que el vector $y=[y_i]$ es un vector de realizaciones de variables aleatorias normales independientes, la densidad conjunta será el producto de las densidades:\n",
        "\n",
        "$p(y)=p(y_1)\\dots p(y_n)=Π\\frac{1}{\\sqrt{2\\pi}σ}e^{-\\frac{(y_i-x_iβ)^2}{2σ}}$\n",
        "\n",
        "\n",
        "Vemos que la densidad conjunta depende de las betas, los datos y además la varianza del modelo. El objetivo es maximizar el valor de la densidad conjunta en función de las betas y la varianza.\n",
        "En vez de maximizar esa función, resulta más operativo maximizar el logaritmo de esa función, ya que es más fácil de tratar analíticamente\n",
        "\n",
        "$\\log{p(y)}=\\sum{\\log{\\frac{1}{\\sqrt{2\\pi}σ}}}-\\sum{\\frac{(y_i-x_iβ)^2}{2σ}}$\n",
        "\n",
        "\n",
        "Cambiando el signo, en vez de maximizar podemos minimizar. Entonces buscamos los betas y la sigma que minimizan la siguiente función:\n",
        "\n",
        "$LV(β,σ)=2n\\log{\\sigma^2}+\\sum{\\frac{(y_i-x_iβ)^2}{2σ}}$\n",
        "\n",
        "Para encontrar los parámetros (betas y desviación típica) que minimizan esa función podemos derivar respecto de los parámetros e igualar a cero. Encontraremos las mismas ecuaciones que las de mínimos cuadrados más otra ecuación para calcular la desviación típica.\n",
        "\n",
        "El valor de la desviación típica que se obtiene por el método de la máxima verosimilitud está ligeramente sesgado a la baja. Normalmente se toma como estimador de la varianza la suma de los errores al cuadrado dividido por el número de grados de libertad del modelo\n",
        "\n",
        "Varianza estimada por MV:   $σ^2=\\frac{\\sum{e_i^2}}{n}$\n",
        "\n",
        "Varianza estimada sin sesgo:   $σ^2=\\frac{\\sum{e_i^2}}{n-k-1}$\n",
        "\n",
        "Puede observarse que la variable estimada por MV es ligeramente inferior a la varianza estimada no sesgada"
      ],
      "metadata": {
        "id": "KBgDKwBCAYAC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KqH5vTA3AjsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluación del modelo\n",
        "\n",
        "Una vez estimado el modelo por cualquiera de los métodos anteriores nos podemos preguntar ¿el modelo calculado es bueno o es malo?\n",
        "\n",
        "Nótese que a partir de un conjunto de datos, **siempre** se puede obtener un modelo de regresión lineal, es decir, siempre podremos encontrar las betas. Ahora bien, aunque las betas obtenidas sean los valores óptimos de una función, eso no quiere decir que el modelo sea bueno.\n",
        "\n",
        "Por lo tanto necesitamos alguna forma  para evaluar si el modelo obtenido es \"bueno\" o \"malo\". Para valorar la bondad del modelo podemos seguir varias estrategias.\n",
        "\n",
        "**Muestra de entrenamiento y muestra de test**\n",
        "\n",
        "Una de las formas más habituales hoy en día es dividir la muestra (el dataset) en dos partes, el training-set y el test-set. Para estimar el modelo se utilza el training-set y para evaluar el modelo se utiliza el test-set. Es un sistema muy intuitivo y de sentido común. Si el modelo hace predicciones razonablemente buenas con los datos del test-set, podemos concluir que el modelo generaliza bien y por lo tanto es bueno. La comprobación es muy simple, calculamos la predicción del modelo sobre los datos del test-set y lo comparamos con su valor verdadero para ver el error que comete el modelo cuando utilizamos observaciones \"nuevas\", es decir, cuando usamos las observaciones del test-set. El analista tiene que valorar si el error del modelo es asumible para el uso que quiera darle al modelo.\n",
        "\n",
        "Este método puede sofisticarse un poco más utilizando la técnica de validación cruzada o \"cross validation\" que consiste en estimar varios modelos con el mismo dataset pero considerando varias divisiones de la muestra en training-set y test-set. Para cada división trainong-set test-set obtendríamos un modelo diferente. A la vista de todos los modelos, el analista puede hacer una valoración del comportamiento del modelo lineal.\n",
        "\n",
        "El método de validación cruzada nos da varios modelos (uno por cada división en training-set / test-set). Por lo tanto si queremos hacer predicciones tendremos varias predicciones, una por cada modelo. ¿Cómo elegir una?. Normalmente se calcula la predicción con todos los modelos y se calcula la predicción promedio.  \n",
        "\n",
        "**Modelo probabilístico**\n",
        "\n",
        "Cuando suponemos que los datos provienen de una población con unas características deterinadas, es decir, cuando asumimos que los datos siguen una determinada distribución, lo primero que tenemos que preguntarnos es ¿Tiene sentido hacer esa suposición?\n",
        "\n",
        "Para valorar si la suposición tiene sentido o no, analizaremos los residuos. Si los residuos siguen una distribución normal, entonces podemos concluir que el modelo lineal se cumple. Si los residuos NO tienen una distribución normal, entonces la hipótesis de la existencia del modelo lineal deja de ser creible\n",
        "\n"
      ],
      "metadata": {
        "id": "cjPcq4FfAkIs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Los coeficientes de regresión\n",
        "\n",
        "Los coeficientes de regresión, es decir, las betas no son observables directamnte. Las betas que calculamos (por mínimos cuadrados o por proyección ortogonal) son meras estimaciones de unas betas \"verdaderas\". Se les llama betas estimadas y se les suele poner un acento circunflejo encima, $\\hat{β}$.\n",
        "\n",
        "Las betas se calculan con los datos observados, por lo tanto, en la medida en que los datos obsevados contienen una componente aleatoria (el error del modelo), son también variables aleatorias. Podemos decir que si observáramos otra muestra obtendríamos otros betas, aunque todas las muestras provengan de la misma población, distintas muestras nos llevarían a estimar distintos betas.\n",
        "\n",
        "Entonces ¿Cómo podemos conocer las betas \"verdaderas\"?\n",
        "\n",
        "En realidad no podemos conocerlas. Los único que podemos averiguar es la distribución de sus estimadores y a partir de esa información calcular intervalos de confianza para la beta \"verdadera\".\n",
        "\n",
        "Los intervalos de confianza se construyen así\n",
        "\n",
        "$β_i$ está en el intervalo $\\hat{β}±t_{n-k-1,α/2}\\times se$\n",
        "\n",
        "Donde $t_{n-k-1,α/2}$ es el valor de la variable aleatoria t-student con n-k-1 grados de libertad que cumple que\n",
        "\n",
        "$Pr(|T|>t_{n-k-1,α/2})=α $\n",
        "\n",
        "El valor de α se llama nivel de significatividad y al valor de $1-α$ nivel de confianza\n",
        "\n",
        "\n",
        "Una de cuestiones más relevantes a contrastar es si un regresor tiene influencia o no en la variable dependiente, lo cual es equivalente a contrastar si la beta \"verdadera\" es igual a cero o es distinta de cero.\n",
        "\n",
        "Evidentenmente si el valor cero está contenido en el intervalo de confianza, no podremos rechazar la hipótesis nula, es decir, no poderemos rechazar que la beta \"verdadera\" vale cero, es decir, el coeficiente NO es significativo\n",
        "\n",
        "Este contraste se hace estableciendo la hipótesis nula H0:\"El coeficiente beta es igual a cero\". Rechazaremos la hipótesis nula si la probabilidad de que suceda algo así es muy baja\n",
        "\n",
        "Finalmente se analiza el p-valor. El p-valor de un estadístico es la probabilidad de obtener el valor concreto que se ha obtenido bajo la hipótesis nula\n",
        "\n",
        "Por ejemplo, si el p-valor del estimador $\\hat{β}_i$ es $0,05$ podemos afirmar que la probabilidad de obtener ese valor es muy baja (5%) bajo la hipótesis nula H0: \"Beta es igual a cero\". Como esa probabilidad es muy baja, rechazamos la hipótesis nula con un nivel de confianza del 95%, o lo que es lo mismo, el parámetro es significativo al 5%\n",
        "\n",
        "En el caso de las betas, los contrastes se hacen a \"dos colas\", quiere decir que se considera el p-valor como la probabilidad de obtener el módulo del estadístico\n",
        "\n"
      ],
      "metadata": {
        "id": "IpJhC8AABHqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Los errores\n",
        "\n",
        "Otro aspecto muy importante para valorar un modelo es verificar que los errores tienen una distribución normal\n",
        "\n",
        "Lo más práctico es hacer un histograma y verificar visualmente que los errores tienen una distribución muestral\n",
        "\n",
        "Adicionalmente existe una amplia variedad de tests para verificar analíticamente si los errores tienen una distribución normal\n",
        "\n"
      ],
      "metadata": {
        "id": "WT6v9dy2BaGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Valoracion global del modelo\n",
        "\n",
        "Otro análsis que se suele realizar es valorar si todas las betas son iguales a cero o si existe alguna distinta de cero. Este contraste se realiza con la F-snedecor, F\n",
        "\n",
        "F es un estadístico (una variable aleatoria) que tiene una distribución conocida. Si el estadístco es mayor de 3 o 4 (dependiendo de los grados de libertad) entonces se puede rechazar la hipótesis nula H0: \"Todos los coeficientes son iguales a cero\".\n",
        "\n",
        "El estadístico F se calcula de la siguiente forma:\n",
        "\n",
        "\n",
        "$F=\\frac{MSS/k}{RSS/(n-k-1)}=\\frac{MSS/k}{\\sigma^2}$\n",
        "\n"
      ],
      "metadata": {
        "id": "RacfxGgJBd7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## El problema de la multicolinealidad\n",
        "\n",
        "Finalmente es importante valorar el grado de multicolinealidad existente entre las variables explicativas. Idealmente las variables explicativas deberían ser independientes, es decir, la correlación entre ellas debería ser cero, o lo que es lo mismo, la matriz de covarianzas deberia ser diagonal (considerando solo las variables explicativas)\n",
        "\n",
        "La multicolinialidad hace que la estimación de las betas sea muy sensible a la muestra considerada, o lo que es lo mismo, los errores en la estimación sean muy elevados.\n",
        "\n",
        "Podemos encontrarnos con la aparente paradoja de tener un buen modelo de regresión (un modelo que hace buenas predicciones) pero con unas betas NO significativas. ¿Cómo es posible?\n",
        "\n",
        "Supongamos un ejemplo extremo en el que dos variables explicativas contienen la misma información pero en distintas unidades. Por ejemplo el peso lo podemos medir en kg o en libras. Sabemos que la relación es 2,2 libras por kg. Si hacemos una regresión de la altura de las personas en función del peso, pero considerando el peso en libras y el peso en kg, podremos encontrar muchas betas optimas, ya que podemos explicar la altura de una persona de igual forma en función de su peso en kg o en libras, o como una combinación del peso medido en ambas unidades.\n",
        "\n",
        "En los casos en lso que la colinealidad no es perfecta, se produce el mismo fenómeno y las betas son muy sensibles a los valores de la muestra."
      ],
      "metadata": {
        "id": "xAnz62-aBxvS"
      }
    }
  ]
}